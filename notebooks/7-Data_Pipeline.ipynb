{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import math\n",
    "import scipy\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/features')\n",
    "\n",
    "from src.features.DataTransformation import LowPassFilter , PrincipalComponentAnalysis\n",
    "from src.features.TemporalAbstraction import NumericalAbstraction\n",
    "from src.features.FrequencyAbstraction import FourierTransformation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor  # pip install scikit-learn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score , RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \n",
    "    def __init__(self, acc_path,gyr_path):\n",
    "        self.acc_path = acc_path\n",
    "        self.gyr_path = gyr_path\n",
    "        self.predicted_exersice = None\n",
    "\n",
    "    def read_data_from_files(self):\n",
    "        acc_df = pd.read_csv(self.acc_path)\n",
    "        gyr_df = pd.read_csv(self.gyr_path)\n",
    "        \n",
    "        acc_df.index = pd.to_datetime(acc_df[\"epoch (ms)\"], unit=\"ms\")\n",
    "        gyr_df.index = pd.to_datetime(gyr_df[\"epoch (ms)\"], unit=\"ms\")\n",
    "        \n",
    "        del acc_df[\"epoch (ms)\"]\n",
    "        del acc_df[\"time (01:00)\"]\n",
    "        del acc_df[\"elapsed (s)\"]\n",
    "\n",
    "        del gyr_df[\"epoch (ms)\"]\n",
    "        del gyr_df[\"time (01:00)\"]\n",
    "        del gyr_df[\"elapsed (s)\"]\n",
    "\n",
    "        acc_set = 1\n",
    "        gyr_set = 1\n",
    "\n",
    "\n",
    "        acc_label = self.acc_path.split(\"-\")[1]\n",
    "        gyr_label = self.gyr_path.split(\"-\")[1]\n",
    "\n",
    "\n",
    "        acc_df[\"label\"] = acc_label\n",
    "        gyr_df[\"label\"] = gyr_label\n",
    "        \n",
    "        if \"Accelerometer\" in self.acc_path:\n",
    "            acc_df[\"set\"] = acc_set\n",
    "            acc_set+= 1\n",
    "            \n",
    "        if \"Gyroscope\" in self.gyr_path:\n",
    "            gyr_df[\"set\"] = gyr_set\n",
    "            gyr_set+= 1\n",
    "        \n",
    "\n",
    "        return acc_df, gyr_df\n",
    "\n",
    "    def merge_and_clean_data(self):\n",
    "        \n",
    "        acc_df, gyr_df = self.read_data_from_files()\n",
    "        \n",
    "        data_merged = pd.concat([acc_df.iloc[:, :3], gyr_df], axis=1)\n",
    "\n",
    "        data_merged.columns = [\n",
    "            \"acc_x\",\n",
    "            \"acc_y\",\n",
    "            \"acc_z\",\n",
    "            \"gyr_x\",\n",
    "            \"gyr_y\",\n",
    "            \"gyr_z\",\n",
    "            \"label\",\n",
    "            \"set\",\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        sampling = {\n",
    "            'acc_x':\"mean\",\n",
    "            'acc_y':\"mean\",\n",
    "            'acc_z':\"mean\",\n",
    "            'gyr_x':\"mean\",\n",
    "            'gyr_y':\"mean\",\n",
    "            'gyr_z':\"mean\",\n",
    "            'label':\"last\",\n",
    "            'set':\"last\",\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Resampling the first 100 data points to a 200ms frequency and applying the defined sampling methods.\n",
    "        # This is done to illustrate the resampling process.\n",
    "        days = [g for n,g in data_merged.groupby(pd.Grouper(freq=\"D\"))]\n",
    "        data_resampled = pd.concat([df.resample(rule=\"200ms\").apply(sampling).dropna() for df in days])\n",
    "        #del data_resampled[\"set\"]\n",
    "       \n",
    "        return data_resampled\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def remove_outliers(self):\n",
    "        # Insert Chauvenet's function\n",
    "        def mark_outliers_chauvenet(dataset, col, C=2):\n",
    "            \"\"\"Finds outliers in the specified column of datatable and adds a binary column with\n",
    "            the same name extended with '_outlier' that expresses the result per data point.\n",
    "            \n",
    "            Taken from: https://github.com/mhoogen/ML4QS/blob/master/Python3Code/Chapter3/OutlierDetection.py\n",
    "\n",
    "            Args:\n",
    "                dataset (pd.DataFrame): The dataset\n",
    "                col (string): The column you want apply outlier detection to\n",
    "                C (int, optional): Degree of certainty for the identification of outliers given the assumption \n",
    "                                of a normal distribution, typicaly between 1 - 10. Defaults to 2.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: The original dataframe with an extra boolean column \n",
    "                indicating whether the value is an outlier or not.\n",
    "            \"\"\"\n",
    "\n",
    "            dataset = dataset.copy()\n",
    "            # Compute the mean and standard deviation.\n",
    "            mean = dataset[col].mean()\n",
    "            std = dataset[col].std()\n",
    "            N = len(dataset.index)\n",
    "            criterion = 1.0 / (C * N)\n",
    "\n",
    "            # Consider the deviation for the data points.\n",
    "            deviation = abs(dataset[col] - mean) / std\n",
    "\n",
    "            # Express the upper and lower bounds.\n",
    "            low = -deviation / math.sqrt(C)\n",
    "            high = deviation / math.sqrt(C)\n",
    "            prob = []\n",
    "            mask = []\n",
    "\n",
    "            # Pass all rows in the dataset.\n",
    "            for i in range(0, len(dataset.index)):\n",
    "                # Determine the probability of observing the point\n",
    "                prob.append(\n",
    "                    1.0 - 0.5 * (scipy.special.erf(high[i]) - scipy.special.erf(low[i]))\n",
    "                )\n",
    "                # And mark as an outlier when the probability is below our criterion.\n",
    "                mask.append(prob[i] < criterion)\n",
    "            dataset[col + \"_outlier\"] = mask\n",
    "            return dataset\n",
    "\n",
    "        df = self.merge_and_clean_data()\n",
    "        oultier_cols = list(df.columns[:6])\n",
    "        \n",
    "        chauvenet_oultier_removed_df = df.copy()\n",
    "\n",
    "        # Iterate through outlier columns\n",
    "        for col in oultier_cols:\n",
    "            # Iterate through unique labels\n",
    "            for label in df[\"label\"].unique():\n",
    "                # Mark outliers using Chauvenet's criterion\n",
    "                dataset = mark_outliers_chauvenet(df[df[\"label\"] == label],col)\n",
    "                # Set outliers to NaN\n",
    "                dataset.loc[dataset[col + \"_outlier\"], col] = np.nan\n",
    "                # Update the outlier-removed DataFrame\n",
    "                chauvenet_oultier_removed_df.loc[(chauvenet_oultier_removed_df[\"label\"] == label ), col] = dataset[col]\n",
    "            \n",
    "        return chauvenet_oultier_removed_df\n",
    "    \n",
    "    \n",
    "    def low_pass(self):\n",
    "        df = self.remove_outliers()\n",
    "        predictor_columns = list(df.columns[:6])\n",
    "        # We try removing all NaN values and filling gaps with the default method, which is linear interpolation. \n",
    "        # For each predictor column, we perform interpolation to fill the missing values. \n",
    "        # By default, interpolation computes the mean between the next and last available values in the same row.\n",
    "        for i in predictor_columns:\n",
    "            df[i] = df[i].interpolate()\n",
    "            \n",
    "        lowpass_df = df.copy()\n",
    "        LowPass = LowPassFilter()\n",
    "        \n",
    "        freq_sample  = 1000 / 200 \n",
    "        cutoff = 1.3 \n",
    "        # Apply the lowpass filter to all predictor columns\n",
    "        for col in predictor_columns:\n",
    "            lowpass_df = LowPass.low_pass_filter(lowpass_df, col, freq_sample , cutoff, order=5)\n",
    "            lowpass_df[col] = lowpass_df[col + \"_lowpass\"]\n",
    "            del lowpass_df[col + \"_lowpass\"]\n",
    "        \n",
    "        return lowpass_df\n",
    "    \n",
    "    def pca(self):\n",
    "        pca_df = self.low_pass().copy()\n",
    "        predictor_columns = list(pca_df.columns[:6])\n",
    "        pca = PrincipalComponentAnalysis()\n",
    "        pca_df = pca.apply_pca(pca_df, predictor_columns, 3)\n",
    "        return pca_df\n",
    "    \n",
    "    def sum_square(self):\n",
    "        squares_df = self.pca().copy()\n",
    "        # Calculate the squared magnitude for accelerometer and gyroscope readings\n",
    "        acc_r = squares_df['acc_x']**2 + squares_df['acc_y']**2 + squares_df['acc_z']**2 \n",
    "        gyr_r = squares_df['gyr_x']**2 + squares_df['gyr_y']**2 + squares_df['gyr_z']**2 \n",
    "\n",
    "        # Calculate the magnitude by taking the square root of the sum of squared components\n",
    "        squares_df['acc_r'] = np.sqrt(acc_r)\n",
    "        squares_df['gyr_r'] = np.sqrt(gyr_r)\n",
    "        return squares_df\n",
    "    \n",
    "    def temporal_abstraction(self):\n",
    "        temporal_df = self.sum_square().copy()\n",
    "        predictor_columns = list(temporal_df.columns[:6])\n",
    "        # Add magnitude columns 'acc_r' and 'gyr_r' to the predictor columns list\n",
    "        predictor_columns = predictor_columns + ['acc_r', 'gyr_r']\n",
    "\n",
    "        # Instantiate the NumericalAbstraction class\n",
    "        NumAbs = NumericalAbstraction()\n",
    "\n",
    "        # Initialize a list to store temporally abstracted subsets\n",
    "        temporal_df_list = []\n",
    "\n",
    "        # Iterate over unique sets in the DataFrame\n",
    "        for set_id in temporal_df['set'].unique():\n",
    "            # Select subset corresponding to the current set\n",
    "            subset = temporal_df[temporal_df['set'] == set_id].copy()\n",
    "            \n",
    "            # Perform temporal abstraction for each predictor column\n",
    "            for col in predictor_columns:\n",
    "                # Calculate the mean and standard deviation with a window size of 5\n",
    "                subset = NumAbs.abstract_numerical(subset, predictor_columns, window_size=5, aggregation_function='mean')\n",
    "                subset = NumAbs.abstract_numerical(subset, predictor_columns, window_size=5, aggregation_function='std')\n",
    "\n",
    "            # Append the abstracted subset to the list\n",
    "            temporal_df_list.append(subset)\n",
    "\n",
    "        # Concatenate all abstracted subsets into a single DataFrame\n",
    "        temporal_df = pd.concat(temporal_df_list)\n",
    "        \n",
    "        return temporal_df\n",
    "    \n",
    "    def frequency(self):\n",
    "        frequency_df = self.temporal_abstraction().copy().reset_index()\n",
    "        predictor_columns = list(frequency_df.columns[:6])\n",
    "        predictor_columns = predictor_columns + ['acc_r', 'gyr_r']\n",
    "        # Instantiate the FourierTransformation class\n",
    "        FreqAbs = FourierTransformation()\n",
    "\n",
    "        # Define the sampling frequency (fs) and window size (ws)\n",
    "        fs = int(1000 / 200)  # Sampling frequency (samples per second)\n",
    "        ws = int(2800 / 200)   # Window size (number of samples)\n",
    "\n",
    "        # Initialize a list to store Fourier-transformed subsets\n",
    "        frequency_df_list = []\n",
    "\n",
    "        # Iterate over unique sets in the DataFrame\n",
    "        for s in frequency_df[\"set\"].unique():\n",
    "            # Select subset corresponding to the current set\n",
    "            subset = frequency_df[frequency_df[\"set\"] == s].reset_index(drop=True).copy()\n",
    "            # Apply Fourier transformations to predictor columns\n",
    "            subset = FreqAbs.abstract_frequency(subset, predictor_columns, ws, fs)\n",
    "            # Append the transformed subset to the list\n",
    "            frequency_df_list.append(subset)\n",
    "\n",
    "        # Concatenate all transformed subsets into a single DataFrame and set index\n",
    "        frequency_df = pd.concat(frequency_df_list).set_index(\"epoch (ms)\", drop=True)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Dealing with overlapping windows\n",
    "        # --------------------------------------------------------------\n",
    "        # Remove rows with any NaN values from the DataFrame\n",
    "        frequency_df = frequency_df.dropna()\n",
    "\n",
    "        # Select every other row in the DataFrame\n",
    "        frequency_df = frequency_df.iloc[::2]\n",
    "        \n",
    "        return frequency_df\n",
    "    \n",
    "    def clusters(self):\n",
    "        cluster_df = self.frequency().copy()\n",
    "        # Perform KMeans clustering with optimal number of clusters (e.g., 5)\n",
    "        cluster_columns = ['acc_x', 'acc_y', 'acc_z']\n",
    "        kmeans = KMeans(n_clusters=5, n_init=20, random_state=0)\n",
    "        subset = cluster_df[cluster_columns]\n",
    "        # Predict cluster labels and assign them to the DataFrame\n",
    "        cluster_df[\"cluster_label\"] = kmeans.fit_predict(subset)\n",
    "                    \n",
    "        return cluster_df\n",
    "\n",
    "    def model(self):\n",
    "        selected_features = ['acc_y_temp_mean_ws_5',\n",
    "                            'acc_y_freq_0.0_Hz_ws_14',\n",
    "                            'cluster_label',\n",
    "                            'acc_z_freq_0.0_Hz_ws_14',\n",
    "                            'pca_1',\n",
    "                            'gyr_r_freq_0.0_Hz_ws_14',\n",
    "                            'acc_y',\n",
    "                            'acc_x_freq_0.0_Hz_ws_14',\n",
    "                            'acc_z_temp_mean_ws_5',\n",
    "                            'acc_z',\n",
    "                            'acc_x_temp_mean_ws_5',\n",
    "                            'gyr_z_temp_std_ws_5',\n",
    "                            'pca_2',\n",
    "                            'acc_z_pse',\n",
    "                            'gyr_r_temp_mean_ws_5']\n",
    "\n",
    "        input_data = self.clusters()[selected_features]\n",
    "        \n",
    "        with open(\"../models/04_Model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        model_output = model.predict(input_data)\n",
    "        self.predicted_exersice = model_output[0]\n",
    "        return model_output[0]\n",
    "        \n",
    "    def count_rep(self):\n",
    "        \n",
    "        exercise = self.predicted_exersice\n",
    "        df = self.merge_and_clean_data()\n",
    "\n",
    "        \n",
    "        acc_r = df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2 \n",
    "        gyr_r = df['gyr_x']**2 + df['gyr_y']**2 + df['gyr_z']**2 \n",
    "\n",
    "        df['acc_r'] = np.sqrt(acc_r)\n",
    "        df['gyr_r'] = np.sqrt(gyr_r)\n",
    "        \n",
    "        fs = 1000 / 200\n",
    "        LowPass= LowPassFilter()\n",
    "\n",
    "\n",
    "        col = \"acc_r\"\n",
    "\n",
    "        def count_reps(dataset ,cutoff = 0.4 ,order = 10 ,column = \"acc_r\"):\n",
    "            data = LowPass.low_pass_filter(data_table = dataset,col = column,\n",
    "                                        sampling_frequency = fs,\n",
    "                                        cutoff_frequency = cutoff,order = order)\n",
    "\n",
    "\n",
    "            index = argrelextrema(data[col + \"_lowpass\"].values, np.greater)\n",
    "            peaks = data.iloc[index]\n",
    "            return len(peaks)\n",
    "                \n",
    "        column = \"acc_r\"\n",
    "        cutoff = 0.4\n",
    "        \n",
    "        if exercise == \"squat\":\n",
    "            cutoff = 0.34\n",
    "        \n",
    "        elif exercise == \"row\":\n",
    "            column = \"gyr_X\"\n",
    "            cutoff = 0.65\n",
    "        \n",
    "        elif exercise == \"ohp\":\n",
    "            cutoff = 0.35\n",
    "            \n",
    "        reps = count_reps(df, cutoff=cutoff, column=column)\n",
    "                \n",
    "        return reps\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker(\"../data/raw/MetaMotion/MetaMotion/A-bench-heavy2-rpe8_MetaWear_2019-01-11T16.10.08.270_C42732BE255C_Accelerometer_12.500Hz_1.4.4.csv\",\"../data/raw/MetaMotion/MetaMotion/A-bench-heavy2-rpe8_MetaWear_2019-01-11T16.10.08.270_C42732BE255C_Gyroscope_25.000Hz_1.4.4.csv\")\n",
    "df = tracker.merge_and_clean_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Tracker(\"../data/raw/MetaMotion/MetaMotion/A-bench-heavy2-rpe8_MetaWear_2019-01-11T16.10.08.270_C42732BE255C_Accelerometer_12.500Hz_1.4.4.csv\",\"../data/raw/MetaMotion/MetaMotion/A-bench-heavy2-rpe8_MetaWear_2019-01-11T16.10.08.270_C42732BE255C_Gyroscope_25.000Hz_1.4.4.csv\")\n",
    "cluster_df = tr.clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model = Tracker(\"../data/raw/MetaMotion/MetaMotion/E-squat-heavy_MetaWear_2019-01-15T20.14.03.633_C42732BE255C_Accelerometer_12.500Hz_1.4.4.csv\",\"../data/raw/MetaMotion/MetaMotion/E-squat-heavy_MetaWear_2019-01-15T20.14.03.633_C42732BE255C_Gyroscope_25.000Hz_1.4.4.csv\")\n",
    "model_output = tr_model.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'squat'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_counts = Tracker(\"../data/raw/MetaMotion/MetaMotion/E-squat-heavy_MetaWear_2019-01-15T20.14.03.633_C42732BE255C_Accelerometer_12.500Hz_1.4.4.csv\",\"../data/raw/MetaMotion/MetaMotion/E-squat-heavy_MetaWear_2019-01-15T20.14.03.633_C42732BE255C_Gyroscope_25.000Hz_1.4.4.csv\")\n",
    "counts = tr_counts.count_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking-barbell-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
