{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "#from DataTransformation import LowPassFilter , PrincipalComponentAnalysis\n",
    "#from TemporalAbstraction import NumericalAbstraction\n",
    "#from FrequencyAbstraction import FourierTransformation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor  # pip install scikit-learn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score , RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 3                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 22-12-2022\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# This class removes the high frequency data (that might be considered noise) from the data.\n",
    "# We can only apply this when we do not have missing values (i.e. NaN).\n",
    "class LowPassFilter:\n",
    "    def low_pass_filter(\n",
    "        self,\n",
    "        data_table,\n",
    "        col,\n",
    "        sampling_frequency,\n",
    "        cutoff_frequency,\n",
    "        order=5,\n",
    "        phase_shift=True,\n",
    "    ):\n",
    "        # http://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "        # Cutoff frequencies are expressed as the fraction of the Nyquist frequency, which is half the sampling frequency\n",
    "        nyq = 0.5 * sampling_frequency\n",
    "        cut = cutoff_frequency / nyq\n",
    "\n",
    "        b, a = butter(order, cut, btype=\"low\", output=\"ba\", analog=False)\n",
    "        if phase_shift:\n",
    "            data_table[col + \"_lowpass\"] = filtfilt(b, a, data_table[col])\n",
    "        else:\n",
    "            data_table[col + \"_lowpass\"] = lfilter(b, a, data_table[col])\n",
    "        return data_table\n",
    "\n",
    "\n",
    "# Class for Principal Component Analysis. We can only apply this when we do not have missing values (i.e. NaN).\n",
    "# For this we have to impute these first, be aware of this.\n",
    "class PrincipalComponentAnalysis:\n",
    "\n",
    "    pca = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pca = []\n",
    "\n",
    "    def normalize_dataset(self, data_table, columns):\n",
    "        dt_norm = copy.deepcopy(data_table)\n",
    "        for col in columns:\n",
    "            dt_norm[col] = (data_table[col] - data_table[col].mean()) / (\n",
    "                data_table[col].max()\n",
    "                - data_table[col].min()\n",
    "                # data_table[col].std()\n",
    "            )\n",
    "        return dt_norm\n",
    "\n",
    "    # Perform the PCA on the selected columns and return the explained variance.\n",
    "    def determine_pc_explained_variance(self, data_table, cols):\n",
    "\n",
    "        # Normalize the data first.\n",
    "        dt_norm = self.normalize_dataset(data_table, cols)\n",
    "\n",
    "        # perform the PCA.\n",
    "        self.pca = PCA(n_components=len(cols))\n",
    "        self.pca.fit(dt_norm[cols])\n",
    "        # And return the explained variances.\n",
    "        return self.pca.explained_variance_ratio_\n",
    "\n",
    "    # Apply a PCA given the number of components we have selected.\n",
    "    # We add new pca columns.\n",
    "    def apply_pca(self, data_table, cols, number_comp):\n",
    "\n",
    "        # Normalize the data first.\n",
    "        dt_norm = self.normalize_dataset(data_table, cols)\n",
    "\n",
    "        # perform the PCA.\n",
    "        self.pca = PCA(n_components=number_comp)\n",
    "        self.pca.fit(dt_norm[cols])\n",
    "\n",
    "        # Transform our old values.\n",
    "        new_values = self.pca.transform(dt_norm[cols])\n",
    "\n",
    "        # And add the new ones:\n",
    "        for comp in range(0, number_comp):\n",
    "            data_table[\"pca_\" + str(comp + 1)] = new_values[:, comp]\n",
    "\n",
    "        return data_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 4                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 22-12-2022\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Class to abstract a history of numerical values we can use as an attribute.\n",
    "class NumericalAbstraction:\n",
    "\n",
    "    # This function aggregates a list of values using the specified aggregation\n",
    "    # function (which can be 'mean', 'max', 'min', 'median', 'std')\n",
    "    def aggregate_value(self, aggregation_function):\n",
    "        # Compute the values and return the result.\n",
    "        if aggregation_function == \"mean\":\n",
    "            return np.mean\n",
    "        elif aggregation_function == \"max\":\n",
    "            return np.max\n",
    "        elif aggregation_function == \"min\":\n",
    "            return np.min\n",
    "        elif aggregation_function == \"median\":\n",
    "            return np.median\n",
    "        elif aggregation_function == \"std\":\n",
    "            return np.std\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    # Abstract numerical columns specified given a window size (i.e. the number of time points from\n",
    "    # the past considered) and an aggregation function.\n",
    "    def abstract_numerical(self, data_table, cols, window_size, aggregation_function):\n",
    "\n",
    "        # Create new columns for the temporal data, pass over the dataset and compute values\n",
    "        for col in cols:\n",
    "            data_table[\n",
    "                col + \"_temp_\" + aggregation_function + \"_ws_\" + str(window_size)\n",
    "            ] = (\n",
    "                data_table[col]\n",
    "                .rolling(window_size)\n",
    "                .apply(self.aggregate_value(aggregation_function))\n",
    "            )\n",
    "\n",
    "        return data_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 4                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 06-01-2023\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# This class performs a Fourier transformation on the data to find frequencies that occur\n",
    "# often and filter noise.\n",
    "class FourierTransformation:\n",
    "\n",
    "    # Find the amplitudes of the different frequencies using a fast fourier transformation. Here,\n",
    "    # the sampling rate expresses the number of samples per second (i.e. Frequency is Hertz of the dataset).\n",
    "    def find_fft_transformation(self, data, sampling_rate):\n",
    "        # Create the transformation, this includes the amplitudes of both the real\n",
    "        # and imaginary part.\n",
    "        transformation = np.fft.rfft(data, len(data))\n",
    "        return transformation.real, transformation.imag\n",
    "\n",
    "    # Get frequencies over a certain window.\n",
    "    def abstract_frequency(self, data_table, cols, window_size, sampling_rate):\n",
    "\n",
    "        # Create new columns for the frequency data.\n",
    "        freqs = np.round((np.fft.rfftfreq(int(window_size)) * sampling_rate), 3)\n",
    "\n",
    "        for col in cols:\n",
    "            data_table[col + \"_max_freq\"] = np.nan\n",
    "            data_table[col + \"_freq_weighted\"] = np.nan\n",
    "            data_table[col + \"_pse\"] = np.nan\n",
    "            for freq in freqs:\n",
    "                data_table[\n",
    "                    col + \"_freq_\" + str(freq) + \"_Hz_ws_\" + str(window_size)\n",
    "                ] = np.nan\n",
    "\n",
    "        # Pass over the dataset (we cannot compute it when we do not have enough history)\n",
    "        # and compute the values.\n",
    "        for i in range(window_size, len(data_table.index)):\n",
    "            for col in cols:\n",
    "                real_ampl, imag_ampl = self.find_fft_transformation(\n",
    "                    data_table[col].iloc[\n",
    "                        i - window_size : min(i + 1, len(data_table.index))\n",
    "                    ],\n",
    "                    sampling_rate,\n",
    "                )\n",
    "                # We only look at the real part in this implementation.\n",
    "                for j in range(0, len(freqs)):\n",
    "                    data_table.loc[\n",
    "                        i, col + \"_freq_\" + str(freqs[j]) + \"_Hz_ws_\" + str(window_size)\n",
    "                    ] = real_ampl[j]\n",
    "                # And select the dominant frequency. We only consider the positive frequencies for now.\n",
    "\n",
    "                data_table.loc[i, col + \"_max_freq\"] = freqs[\n",
    "                    np.argmax(real_ampl[0 : len(real_ampl)])\n",
    "                ]\n",
    "                data_table.loc[i, col + \"_freq_weighted\"] = float(\n",
    "                    np.sum(freqs * real_ampl)\n",
    "                ) / np.sum(real_ampl)\n",
    "                PSD = np.divide(np.square(real_ampl), float(len(real_ampl)))\n",
    "                PSD_pdf = np.divide(PSD, np.sum(PSD))\n",
    "                data_table.loc[i, col + \"_pse\"] = -np.sum(np.log(PSD_pdf) * PSD_pdf)\n",
    "\n",
    "        return data_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tracker:\n",
    "    def __init__(self, files_path):\n",
    "        self.files = glob(files_path)\n",
    "\n",
    "    def read_data_from_files(self):\n",
    "        acc_df = pd.DataFrame()\n",
    "        gyr_df = pd.DataFrame()\n",
    "\n",
    "        acc_set = 1\n",
    "        gyr_set = 1\n",
    "\n",
    "        for f in self.files:\n",
    "            label = f.split(\"-\")[1]\n",
    "\n",
    "            df = pd.read_csv(f)\n",
    "\n",
    "            df[\"label\"] = label\n",
    "            \n",
    "            if \"Accelerometer\" in f:\n",
    "                df[\"set\"] = acc_set\n",
    "                acc_set+= 1\n",
    "                acc_df = pd.concat([acc_df, df])\n",
    "            else:\n",
    "                df[\"set\"] = gyr_set\n",
    "                gyr_set+= 1\n",
    "                gyr_df = pd.concat([gyr_df, df])\n",
    "\n",
    "        acc_df.index = pd.to_datetime(acc_df[\"epoch (ms)\"], unit=\"ms\")\n",
    "        gyr_df.index = pd.to_datetime(gyr_df[\"epoch (ms)\"], unit=\"ms\")\n",
    "\n",
    "        del acc_df[\"epoch (ms)\"]\n",
    "        del acc_df[\"time (01:00)\"]\n",
    "        del acc_df[\"elapsed (s)\"]\n",
    "\n",
    "        del gyr_df[\"epoch (ms)\"]\n",
    "        del gyr_df[\"time (01:00)\"]\n",
    "        del gyr_df[\"elapsed (s)\"]\n",
    "\n",
    "        return acc_df, gyr_df\n",
    "\n",
    "    def merge_and_clean_data(self):\n",
    "        \n",
    "        acc_df, gyr_df = self.read_data_from_files()\n",
    "        \n",
    "        data_merged = pd.concat([acc_df.iloc[:, :3], gyr_df], axis=1)\n",
    "\n",
    "        data_merged.columns = [\n",
    "            \"acc_x\",\n",
    "            \"acc_y\",\n",
    "            \"acc_z\",\n",
    "            \"gyr_x\",\n",
    "            \"gyr_y\",\n",
    "            \"gyr_z\",\n",
    "            \"label\",\n",
    "            \"set\",\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        sampling = {\n",
    "            'acc_x':\"mean\",\n",
    "            'acc_y':\"mean\",\n",
    "            'acc_z':\"mean\",\n",
    "            'gyr_x':\"mean\",\n",
    "            'gyr_y':\"mean\",\n",
    "            'gyr_z':\"mean\",\n",
    "            'label':\"last\",\n",
    "            'set':\"last\",\n",
    "            \n",
    "            \n",
    "        }\n",
    "\n",
    "        # Resampling the first 100 data points to a 200ms frequency and applying the defined sampling methods.\n",
    "        # This is done to illustrate the resampling process.\n",
    "        data_merged[:100].resample(rule=\"200ms\").apply(sampling)\n",
    "        days = [g for n,g in data_merged.groupby(pd.Grouper(freq=\"D\"))]\n",
    "        data_resampled = pd.concat([df.resample(rule=\"200ms\").apply(sampling).dropna() for df in days])\n",
    "        #del data_resampled[\"set\"]\n",
    "       \n",
    "        return data_resampled\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def remove_outliers(self):\n",
    "        # Insert Chauvenet's function\n",
    "        def mark_outliers_chauvenet(dataset, col, C=2):\n",
    "            \"\"\"Finds outliers in the specified column of datatable and adds a binary column with\n",
    "            the same name extended with '_outlier' that expresses the result per data point.\n",
    "            \n",
    "            Taken from: https://github.com/mhoogen/ML4QS/blob/master/Python3Code/Chapter3/OutlierDetection.py\n",
    "\n",
    "            Args:\n",
    "                dataset (pd.DataFrame): The dataset\n",
    "                col (string): The column you want apply outlier detection to\n",
    "                C (int, optional): Degree of certainty for the identification of outliers given the assumption \n",
    "                                of a normal distribution, typicaly between 1 - 10. Defaults to 2.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: The original dataframe with an extra boolean column \n",
    "                indicating whether the value is an outlier or not.\n",
    "            \"\"\"\n",
    "\n",
    "            dataset = dataset.copy()\n",
    "            # Compute the mean and standard deviation.\n",
    "            mean = dataset[col].mean()\n",
    "            std = dataset[col].std()\n",
    "            N = len(dataset.index)\n",
    "            criterion = 1.0 / (C * N)\n",
    "\n",
    "            # Consider the deviation for the data points.\n",
    "            deviation = abs(dataset[col] - mean) / std\n",
    "\n",
    "            # Express the upper and lower bounds.\n",
    "            low = -deviation / math.sqrt(C)\n",
    "            high = deviation / math.sqrt(C)\n",
    "            prob = []\n",
    "            mask = []\n",
    "\n",
    "            # Pass all rows in the dataset.\n",
    "            for i in range(0, len(dataset.index)):\n",
    "                # Determine the probability of observing the point\n",
    "                prob.append(\n",
    "                    1.0 - 0.5 * (scipy.special.erf(high[i]) - scipy.special.erf(low[i]))\n",
    "                )\n",
    "                # And mark as an outlier when the probability is below our criterion.\n",
    "                mask.append(prob[i] < criterion)\n",
    "            dataset[col + \"_outlier\"] = mask\n",
    "            return dataset\n",
    "\n",
    "        df = self.merge_and_clean_data()\n",
    "        oultier_cols = list(df.columns[:6])\n",
    "        \n",
    "        chauvenet_oultier_removed_df = df.copy()\n",
    "\n",
    "        # Iterate through outlier columns\n",
    "        for col in oultier_cols:\n",
    "            # Iterate through unique labels\n",
    "            for label in df[\"label\"].unique():\n",
    "                # Mark outliers using Chauvenet's criterion\n",
    "                dataset = mark_outliers_chauvenet(df[df[\"label\"] == label],col)\n",
    "                # Set outliers to NaN\n",
    "                dataset.loc[dataset[col + \"_outlier\"], col] = np.nan\n",
    "                # Update the outlier-removed DataFrame\n",
    "                chauvenet_oultier_removed_df.loc[(chauvenet_oultier_removed_df[\"label\"] == label ), col] = dataset[col]\n",
    "            \n",
    "        return chauvenet_oultier_removed_df\n",
    "    \n",
    "    \n",
    "    def low_pass(self):\n",
    "        df = self.remove_outliers()\n",
    "        predictor_columns = list(df.columns[:6])\n",
    "        # We try removing all NaN values and filling gaps with the default method, which is linear interpolation. \n",
    "        # For each predictor column, we perform interpolation to fill the missing values. \n",
    "        # By default, interpolation computes the mean between the next and last available values in the same row.\n",
    "        for i in predictor_columns:\n",
    "            df[i] = df[i].interpolate()\n",
    "            \n",
    "        lowpass_df = df.copy()\n",
    "        LowPass = LowPassFilter()\n",
    "        \n",
    "        freq_sample  = 1000 / 200 \n",
    "        cutoff = 1.3 \n",
    "        # Apply the lowpass filter to all predictor columns\n",
    "        for col in predictor_columns:\n",
    "            lowpass_df = LowPass.low_pass_filter(lowpass_df, col, freq_sample , cutoff, order=5)\n",
    "            lowpass_df[col] = lowpass_df[col + \"_lowpass\"]\n",
    "            del lowpass_df[col + \"_lowpass\"]\n",
    "        \n",
    "        return lowpass_df\n",
    "    \n",
    "    def pca(self):\n",
    "        pca_df = self.low_pass().copy()\n",
    "        predictor_columns = list(pca_df.columns[:6])\n",
    "        pca = PrincipalComponentAnalysis()\n",
    "        pca_df = pca.apply_pca(pca_df, predictor_columns, 3)\n",
    "        return pca_df\n",
    "    \n",
    "    def sum_square(self):\n",
    "        squares_df = self.pca().copy()\n",
    "        # Calculate the squared magnitude for accelerometer and gyroscope readings\n",
    "        acc_r = squares_df['acc_x']**2 + squares_df['acc_y']**2 + squares_df['acc_z']**2 \n",
    "        gyr_r = squares_df['gyr_x']**2 + squares_df['gyr_y']**2 + squares_df['gyr_z']**2 \n",
    "\n",
    "        # Calculate the magnitude by taking the square root of the sum of squared components\n",
    "        squares_df['acc_r'] = np.sqrt(acc_r)\n",
    "        squares_df['gyr_r'] = np.sqrt(gyr_r)\n",
    "        return squares_df\n",
    "    \n",
    "    def temporal_abstraction(self):\n",
    "        temporal_df = self.sum_square().copy()\n",
    "        predictor_columns = list(temporal_df.columns[:6])\n",
    "        # Add magnitude columns 'acc_r' and 'gyr_r' to the predictor columns list\n",
    "        predictor_columns = predictor_columns + ['acc_r', 'gyr_r']\n",
    "\n",
    "        # Instantiate the NumericalAbstraction class\n",
    "        NumAbs = NumericalAbstraction()\n",
    "\n",
    "        # Initialize a list to store temporally abstracted subsets\n",
    "        temporal_df_list = []\n",
    "\n",
    "        # Iterate over unique sets in the DataFrame\n",
    "        for set_id in temporal_df['set'].unique():\n",
    "            # Select subset corresponding to the current set\n",
    "            subset = temporal_df[temporal_df['set'] == set_id].copy()\n",
    "            \n",
    "            # Perform temporal abstraction for each predictor column\n",
    "            for col in predictor_columns:\n",
    "                # Calculate the mean and standard deviation with a window size of 5\n",
    "                subset = NumAbs.abstract_numerical(subset, predictor_columns, window_size=5, aggregation_function='mean')\n",
    "                subset = NumAbs.abstract_numerical(subset, predictor_columns, window_size=5, aggregation_function='std')\n",
    "\n",
    "            # Append the abstracted subset to the list\n",
    "            temporal_df_list.append(subset)\n",
    "\n",
    "        # Concatenate all abstracted subsets into a single DataFrame\n",
    "        temporal_df = pd.concat(temporal_df_list)\n",
    "        \n",
    "        return temporal_df\n",
    "    \n",
    "    def frequency(self):\n",
    "        frequency_df = self.temporal_abstraction().copy().reset_index()\n",
    "        predictor_columns = list(frequency_df.columns[:6])\n",
    "        predictor_columns = predictor_columns + ['acc_r', 'gyr_r']\n",
    "        # Instantiate the FourierTransformation class\n",
    "        FreqAbs = FourierTransformation()\n",
    "\n",
    "        # Define the sampling frequency (fs) and window size (ws)\n",
    "        fs = int(1000 / 200)  # Sampling frequency (samples per second)\n",
    "        ws = int(2800 / 200)   # Window size (number of samples)\n",
    "\n",
    "        # Initialize a list to store Fourier-transformed subsets\n",
    "        frequency_df_list = []\n",
    "\n",
    "        # Iterate over unique sets in the DataFrame\n",
    "        for s in frequency_df[\"set\"].unique():\n",
    "            # Select subset corresponding to the current set\n",
    "            subset = frequency_df[frequency_df[\"set\"] == s].reset_index(drop=True).copy()\n",
    "            # Apply Fourier transformations to predictor columns\n",
    "            subset = FreqAbs.abstract_frequency(subset, predictor_columns, ws, fs)\n",
    "            # Append the transformed subset to the list\n",
    "            frequency_df_list.append(subset)\n",
    "\n",
    "        # Concatenate all transformed subsets into a single DataFrame and set index\n",
    "        frequency_df = pd.concat(frequency_df_list).set_index(\"epoch (ms)\", drop=True)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Dealing with overlapping windows\n",
    "        # --------------------------------------------------------------\n",
    "        # Remove rows with any NaN values from the DataFrame\n",
    "        frequency_df = frequency_df.dropna()\n",
    "\n",
    "        # Select every other row in the DataFrame\n",
    "        frequency_df = frequency_df.iloc[::2]\n",
    "        \n",
    "        # List of column names to drop\n",
    "        columns_to_drop = ['epoch (ms)_max_freq', 'epoch (ms)_freq_weighted', 'epoch (ms)_pse',\n",
    "                        'epoch (ms)_freq_0.0_Hz_ws_14', 'epoch (ms)_freq_0.357_Hz_ws_14',\n",
    "                        'epoch (ms)_freq_0.714_Hz_ws_14', 'epoch (ms)_freq_1.071_Hz_ws_14',\n",
    "                        'epoch (ms)_freq_1.429_Hz_ws_14', 'epoch (ms)_freq_1.786_Hz_ws_14',\n",
    "                        'epoch (ms)_freq_2.143_Hz_ws_14', 'epoch (ms)_freq_2.5_Hz_ws_14']\n",
    "\n",
    "        # Drop the specified columns\n",
    "        frequency_df = frequency_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return frequency_df\n",
    "    \n",
    "    def clusters(self):\n",
    "        cluster_df = self.frequency().copy()\n",
    "        # Perform KMeans clustering with optimal number of clusters (e.g., 5)\n",
    "        cluster_columns = ['acc_x', 'acc_y', 'acc_z']\n",
    "        kmeans = KMeans(n_clusters=5, n_init=20, random_state=0)\n",
    "        subset = cluster_df[cluster_columns]\n",
    "        # Predict cluster labels and assign them to the DataFrame\n",
    "        cluster_df[\"cluster_label\"] = kmeans.fit_predict(subset)\n",
    "                    \n",
    "        return cluster_df\n",
    "    \n",
    "    def model(self):\n",
    "        selected_features = ['acc_y_freq_0.0_Hz_ws_14',\n",
    "                            'acc_z_freq_0.0_Hz_ws_14',\n",
    "                            'pca_1',\n",
    "                            'acc_y_temp_mean_ws_5',\n",
    "                            'cluster_label',\n",
    "                            'acc_y',\n",
    "                            'gyr_r_freq_0.0_Hz_ws_14',\n",
    "                            'pca_2',\n",
    "                            'acc_z_temp_mean_ws_5',\n",
    "                            'acc_x_freq_0.0_Hz_ws_14',\n",
    "                            'acc_z',\n",
    "                            'acc_x_temp_mean_ws_5',\n",
    "                            'gyr_z_temp_std_ws_5',\n",
    "                            'gyr_r_temp_mean_ws_5',\n",
    "                            'acc_y_max_freq']\n",
    "        \n",
    "        input_data = self.clusters()[selected_features]\n",
    "        \n",
    "        with open(\"../models/04_Model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        input_values = input_data.to_numpy().reshape(1, -1)\n",
    "        return model.predict(input_values)\n",
    "    \n",
    "    def count_rep(self):\n",
    "        \n",
    "        exercise = self.model()[0]\n",
    "        df = self.merge_and_clean_data()\n",
    "        \n",
    "        fs = int(1000 / 200)\n",
    "        \n",
    "        column = \"acc_r\"\n",
    "        cutoff = 0.4\n",
    "        \n",
    "        if exercise == \"squat\":\n",
    "            cutoff = 0.34\n",
    "        \n",
    "        elif exercise == \"row\":\n",
    "            cutoff = 0.65\n",
    "            column = 'gyr_x'\n",
    "        \n",
    "        elif exercise == \"ohp\":\n",
    "            cutoff = 0.35\n",
    "            \n",
    "        def count_reps(dataset ,cutoff = 0.4 ,order = 10 ,column = column):\n",
    "            LowPass = LowPassFilter()\n",
    "            data = LowPass.low_pass_filter(data_table = dataset,col = column,\n",
    "                                        sampling_frequency = fs,\n",
    "                                        cutoff_frequency = cutoff,order = order)\n",
    "\n",
    "\n",
    "            index = argrelextrema(data[column + \"_lowpass\"].values, np.greater)\n",
    "            peaks = data.iloc[index]\n",
    "            return len(peaks)\n",
    "        \n",
    "        reps = count_reps(df, cutoff=cutoff, column=column)\n",
    "        \n",
    "        return reps\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker(\"../data/raw/MetaMotion/MetaMotion/*.csv\")\n",
    "df = tracker.clusters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 58020 features, but RandomForestClassifier is expecting 15 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_rep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[24], line 305\u001b[0m, in \u001b[0;36mTracker.count_rep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_rep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     exercise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    306\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_and_clean_data()\n\u001b[0;32m    308\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m200\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 301\u001b[0m, in \u001b[0;36mTracker.model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    300\u001b[0m input_values \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\tracking-barbell-exercises\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\tracking-barbell-exercises\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\tracking-barbell-exercises\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\tracking-barbell-exercises\\lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\envs\\tracking-barbell-exercises\\lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 58020 features, but RandomForestClassifier is expecting 15 features as input."
     ]
    }
   ],
   "source": [
    "print(tracker.count_rep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.45027497e+01 -2.07567846e+00 -3.11963218e-01  1.01146254e+00\n",
      "   1.00000000e+00  9.09032617e-01  2.41915837e+02 -1.67364387e-01\n",
      "  -1.61931081e-01 -1.69685551e+00 -1.68556248e-01 -1.95715115e-01\n",
      "   1.47236123e+01  2.28002285e+01  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['acc_y_freq_0.0_Hz_ws_14',\n",
    "                            'acc_z_freq_0.0_Hz_ws_14',\n",
    "                            'pca_1',\n",
    "                            'acc_y_temp_mean_ws_5',\n",
    "                            'cluster_label',\n",
    "                            'acc_y',\n",
    "                            'gyr_r_freq_0.0_Hz_ws_14',\n",
    "                            'pca_2',\n",
    "                            'acc_z_temp_mean_ws_5',\n",
    "                            'acc_x_freq_0.0_Hz_ws_14',\n",
    "                            'acc_z',\n",
    "                            'acc_x_temp_mean_ws_5',\n",
    "                            'gyr_z_temp_std_ws_5',\n",
    "                            'gyr_r_temp_mean_ws_5',\n",
    "                            'acc_y_max_freq']\n",
    "values = df[selected_features]\n",
    "input_data = values.iloc[0].to_numpy().reshape(1, -1)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bench'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"../models/04_Model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking-barbell-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
